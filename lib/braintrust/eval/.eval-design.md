# Braintrust Ruby SDK - Eval API Design

**Created**: 2025-10-21
**Status**: Design Complete, Ready for Implementation

## Overview

The Eval API provides a framework for evaluating AI model outputs against expected results using custom scoring functions. It handles:
- Running tasks (code being evaluated) on test cases
- Scoring outputs against expected values
- Parallel execution for performance
- Error collection and reporting
- Integration with Braintrust experiments for tracking

## Design Decisions

### Decision 1: Tasks (How Users Define Code Being Evaluated)

**Decision: Hybrid - Accept anything callable**

**Rationale:**
- Maximum flexibility for simple and complex use cases
- Allows inline procs/lambdas for simple tasks
- Supports classes for reusable, configurable tasks
- Very Ruby-idiomatic (like Rack, Rails routing)

**Implementation:**
- Validate with `responds_to?(:call)`
- Task receives one parameter: `input`
- Task returns output value (any type)

**Examples:**
```ruby
# Inline proc
task: ->(input) { classify_food(input) }

# Class with configuration
class APIClassifier
  def initialize(api_key, endpoint)
    @api_key = api_key
    @endpoint = endpoint
  end

  def call(input)
    HTTP.auth("Bearer #{@api_key}")
        .post(@endpoint, json: {text: input})
        .parse["result"]
  end
end

task: APIClassifier.new(ENV["API_KEY"], "https://api.example.com/classify")

# Method reference
task: method(:classify_food)
```

---

### Decision 2: Scorer Parameters (Arity Detection)

**Decision: Optional metadata with arity detection (3 or 4 params)**

**Rationale:**
- Most scorers don't need metadata - keep them simple
- Advanced scorers can access metadata when needed
- Arity detection is idiomatic Ruby (like Rails callbacks)
- Clear intent from parameter count

**Implementation:**
```ruby
case block.arity
when 3
  # Block takes (input, expected, output)
  ->(i, e, o, m) { block.call(i, e, o) }
when 4, -4  # -4 means optional 4th param
  # Block takes (input, expected, output, metadata)
  block
else
  raise ArgumentError, "Scorer block must accept 3 or 4 parameters"
end
```

**Examples:**
```ruby
# Simple - 3 params
Eval.scorer("exact_match") { |input, expected, output|
  output == expected ? 1.0 : 0.0
}

# Advanced - 4 params with metadata
Eval.scorer("threshold_match") { |input, expected, output, metadata|
  threshold = metadata[:threshold] || 0.8
  similarity(output, expected) >= threshold ? 1.0 : 0.0
}
```

---

### Decision 3: Scorer Return Values (Normalize All Formats)

**Decision: Accept float, hash, or array - normalize internally**

**Rationale:**
- Simple case stays simple (return a float)
- Advanced case supported (return multiple scores)
- Progressive complexity - users can grow into features
- Very Ruby - duck typing, "make it work"

**Implementation:**
```ruby
def normalize_scores(result, scorer_name)
  case result
  when Numeric
    [{name: scorer_name, score: result.to_f}]
  when Hash
    name = result[:name] || result["name"] || scorer_name
    score = result[:score] || result["score"]
    [{name: name, score: score.to_f}]
  when Array
    result.map { |r| normalize_scores(r, scorer_name).first }
  else
    raise ArgumentError, "Invalid scorer return value: #{result.class}"
  end
end
```

**Examples:**
```ruby
# Simple - return float
Eval.scorer("exact_match") { |i, e, o|
  o == e ? 1.0 : 0.0
}

# Return hash with custom name
Eval.scorer("similarity") { |i, e, o|
  {name: "cosine_similarity", score: 0.85}
}

# Multiple scores from one scorer
Eval.scorer("nlp_metrics") { |i, e, o|
  [
    {name: "bleu", score: calculate_bleu(e, o)},
    {name: "rouge", score: calculate_rouge(e, o)},
    {name: "meteor", score: calculate_meteor(e, o)}
  ]
}
```

---

### Decision 4: Scorer Definition (Helper Method)

**Decision: `Eval.scorer(name, &block)` helper + class support**

**Rationale:**
- Clean helper for inline scorers (matches Go SDK)
- Supports custom classes for reusable scorers
- Name is explicit and associated with scorer
- Flexible - block or callable argument

**Implementation:**
```ruby
def self.scorer(name, callable = nil, &block)
  scorer_impl = callable || block
  raise ArgumentError, "Must provide callable or block" unless scorer_impl
  Scorer.new(name, scorer_impl)
end

class Scorer
  attr_reader :name

  def initialize(name, callable)
    @name = name
    @callable = callable
  end

  def call(input, expected, output, metadata = {})
    # Handle arity detection and normalization
  end
end
```

**Examples:**
```ruby
scorers: [
  # Block form (inline)
  Eval.scorer("exact_match") { |i, e, o| o == e ? 1.0 : 0.0 },

  # Callable form (reusable class)
  Eval.scorer("fuzzy", FuzzyScorer.new),

  # Or if class already has .name method:
  LLMJudgeScorer.new(ENV["OPENAI_API_KEY"])
]
```

---

### Decision 5: Cases (Test Data Input)

**Decision: Hybrid - Accept Array, Enumerable, or Cases wrapper**

**Rationale:**
- Simple stays simple (array of hashes)
- Advanced supported (lazy enumerators, dataset fetchers)
- Progressive complexity
- Memory efficient for large datasets

**Implementation:**
```ruby
def self.normalize_cases(cases_input)
  case cases_input
  when Array
    # Simple array → wrap in Cases iterator
    Cases.new(cases_input)
  when Cases
    # Already wrapped
    cases_input
  else
    # Assume it's Enumerable (enumerator, dataset, etc.)
    if cases_input.respond_to?(:each)
      Cases.new(cases_input)
    else
      raise ArgumentError, "cases must be Array or Enumerable"
    end
  end
end
```

**Examples:**
```ruby
# Simple - array of hashes
cases: [
  {input: "apple", expected: "fruit"},
  {input: "carrot", expected: "vegetable", tags: ["root"], metadata: {category: "produce"}}
]

# Dataset (lazy loading)
cases: Eval.dataset("my-dataset", project: "my-project")

# Custom enumerator
cases: Enumerator.new do |y|
  CSV.foreach("test_cases.csv") do |row|
    y << {input: row[0], expected: row[1]}
  end
end
```

---

### Decision 6: Result Object

**Decision: Result class with methods**

**Rationale:**
- Explicit interface (IDE autocomplete)
- Methods like `success?`, `failed?`, `to_s`
- Matches Go SDK
- Extensible for future stats/summaries

**Implementation:**
```ruby
class Result
  attr_reader :experiment_id, :experiment_name, :project_id,
              :permalink, :errors, :duration

  def initialize(experiment_id:, experiment_name:, project_id:,
                 permalink:, errors:, duration:)
    @experiment_id = experiment_id
    @experiment_name = experiment_name
    @project_id = project_id
    @permalink = permalink
    @errors = errors
    @duration = duration
  end

  def success?
    errors.empty?
  end

  def failed?
    !success?
  end

  def to_s
    <<~MSG

      === Experiment: #{experiment_name} ===
      Project: #{project_id}
      Duration: #{duration.round(1)}s
      Link: #{permalink}
      #{errors.any? ? "\nErrors:\n  #{errors.join("\n  ")}" : ""}
    MSG
  end
end
```

**Usage:**
```ruby
result = Eval.run(...)
puts result.permalink
puts "Success: #{result.success?}"
puts "Duration: #{result.duration}s"
result.errors.each { |err| puts "Error: #{err}" }
```

---

### Decision 7: Error Handling

**Decision: Collect all errors, don't raise**

**Rationale:**
- See all failures, not just the first one
- Parallel-friendly (other threads continue)
- Matches Go SDK (`errors.Join`)
- User can raise if desired: `raise result.errors.first unless result.success?`

**Implementation:**
```ruby
errors = []

# Collect task errors
begin
  output = task.call(input)
rescue => e
  errors << "Task failed for input '#{input}': #{e.message}"
  next  # Continue to next case
end

# Collect scorer errors
scorers.each do |scorer|
  begin
    score = scorer.call(input, expected, output, metadata)
  rescue => e
    errors << "Scorer '#{scorer.name}' failed: #{e.message}"
    # Continue with other scorers
  end
end

# Return result with all collected errors
Result.new(..., errors: errors)
```

---

### Decision 8: Parallelism

**Decision: Ruby Threads (stdlib)**

**Rationale:**
- No dependencies (stdlib only)
- Good for evals (most are I/O bound - API calls, LLM judge)
- Simple thread pool pattern
- Matches Go's goroutines conceptually

**Implementation:**
```ruby
parallelism = opts[:parallelism] || 1
queue = Queue.new
results = []
mutex = Mutex.new

# Fill queue
cases.each { |test_case| queue << test_case }

# Spawn worker threads
threads = parallelism.times.map do
  Thread.new do
    while (test_case = queue.pop(true) rescue nil)
      result = run_case(test_case)
      mutex.synchronize { results << result }
    end
  end
end

# Wait for all threads
threads.each(&:join)
```

**Usage:**
```ruby
Eval.run(
  ...,
  parallelism: 5  # Run 5 cases concurrently
)
```

---

## Complete API Example

```ruby
result = Eval.run(
  # Required: Project and experiment
  project: "ruby-sdk-examples",
  experiment: "food-classifier-v1",

  # Required: Test cases
  # Simple array of hashes
  cases: [
    {input: "apple", expected: "fruit"},
    {input: "carrot", expected: "vegetable"},
    {input: "banana", expected: "fruit", tags: ["tropical"]},
    {input: "broccoli", expected: "vegetable", metadata: {category: "cruciferous"}}
  ],

  # Required: Task (callable)
  # Can be proc, lambda, or object with .call
  task: ->(input) {
    # Call your model/API/function
    classify_food(input)
  },

  # Required: Scorers (array)
  scorers: [
    # Simple inline scorer (3 params)
    Eval.scorer("exact_match") { |input, expected, output|
      output == expected ? 1.0 : 0.0
    },

    # Advanced scorer with metadata (4 params)
    Eval.scorer("fuzzy_match") { |input, expected, output, metadata|
      threshold = metadata[:threshold] || 0.8
      similarity(output, expected) >= threshold ? 1.0 : 0.0
    },

    # Multi-score scorer (returns array)
    Eval.scorer("nlp_metrics") { |i, e, o|
      [
        {name: "bleu", score: calculate_bleu(e, o)},
        {name: "rouge", score: calculate_rouge(e, o)}
      ]
    },

    # Class-based scorer (reusable)
    LLMJudgeScorer.new(ENV["OPENAI_API_KEY"])
  ],

  # Optional: Parallelism (default: 1)
  parallelism: 5,

  # Optional: Tags for the experiment
  tags: ["example", "food-classifier", "v1"],

  # Optional: Metadata for the experiment
  metadata: {
    model: "gpt-4o-mini",
    version: "1.0.0",
    description: "Food classification eval"
  }
)

# Use the result
puts result.to_s  # Pretty-printed summary
puts result.permalink  # Link to Braintrust UI
puts "Success: #{result.success?}"
puts "Duration: #{result.duration.round(2)}s"

unless result.success?
  puts "\nErrors:"
  result.errors.each { |e| puts "  - #{e}" }
end
```

---

## HTTP/API Usage Examples

### LLM-as-Judge Pattern

```ruby
class LLMJudgeScorer
  def initialize(api_key, model: "gpt-4o-mini")
    @client = OpenAI::Client.new(api_key: api_key)
    @model = model
  end

  def name
    "llm_judge"
  end

  def call(input, expected, output, metadata = {})
    prompt = <<~PROMPT
      Rate the quality of this output on a scale of 0.0 to 1.0.

      Input: #{input}
      Expected: #{expected}
      Got: #{output}

      Return only a number between 0.0 and 1.0.
    PROMPT

    response = @client.chat.completions.create(
      model: @model,
      messages: [{role: "user", content: prompt}],
      max_tokens: 10
    )

    response.choices[0].message.content.to_f
  end
end

# Usage
result = Eval.run(
  project: "my-project",
  experiment: "llm-judge-eval",
  cases: [...],
  task: ->(input) { my_model.generate(input) },
  scorers: [
    LLMJudgeScorer.new(ENV["OPENAI_API_KEY"])
  ]
)
```

### API-Based Task

```ruby
class APIClassifier
  def initialize(api_key, endpoint)
    @api_key = api_key
    @endpoint = endpoint
  end

  def call(input)
    response = HTTP.auth("Bearer #{@api_key}")
                   .post(@endpoint, json: {text: input})
    response.parse["classification"]
  end
end

# Usage
result = Eval.run(
  project: "api-eval",
  experiment: "classification-test",
  cases: [{input: "text", expected: "label"}],
  task: APIClassifier.new(ENV["API_KEY"], "https://api.example.com/classify"),
  scorers: [
    Eval.scorer("exact") { |i, e, o| o == e ? 1.0 : 0.0 }
  ],
  parallelism: 10  # Run 10 API calls concurrently
)
```

---

## Implementation Notes

### Key Classes

1. **Eval** (`lib/braintrust/eval.rb`)
   - Module with `Eval.run` and `Eval.scorer` methods
   - Main entry point for users
   - Handles options parsing and orchestration

2. **Result** (`lib/braintrust/eval/result.rb`)
   - Value object containing eval results
   - Methods: `success?`, `failed?`, `permalink`, `to_s`

3. **Case** (`lib/braintrust/eval/case.rb`)
   - Struct representing a test case
   - Fields: `input`, `expected`, `tags`, `metadata`

4. **Cases** (`lib/braintrust/eval/cases.rb`)
   - Iterator wrapper for test cases
   - Wraps arrays or enumerables
   - Provides `each` method

5. **Scorer** (`lib/braintrust/eval/scorer.rb`)
   - Wrapper for scorer callables
   - Handles arity detection
   - Normalizes return values

### OpenTelemetry Spans

Following Go SDK pattern, create spans for:
- **eval span**: One per test case (parent span)
  - Attributes: `braintrust.input_json`, `braintrust.output_json`, `braintrust.expected`, `braintrust.span_attributes` (type: "eval")
- **task span**: Child of eval span
  - Attributes: `braintrust.input_json`, `braintrust.output_json`, `braintrust.span_attributes` (type: "task")
- **score span**: Child of eval span
  - Attributes: `braintrust.scores` (map of score name → value), `braintrust.span_attributes` (type: "score")

### API Integration

Need to implement API methods:
- `API.register_project(name)` → returns `{id:, name:}`
- `API.register_experiment(name, project_id, opts)` → returns `{id:, name:}`

### Thread Safety

- Use `Queue` for work distribution
- Use `Mutex` for shared state (errors array, results)
- Each thread runs independently on its own case

---

## Future Enhancements

### Dataset Support
```ruby
cases: Eval.dataset("my-dataset", project: "my-project")
```
Lazy-loads dataset from Braintrust API.

### Built-in Scorers
```ruby
Eval::Scorers::ExactMatch.new
Eval::Scorers::Levenshtein.new(threshold: 0.8)
```

### Summary Statistics
```ruby
result.summary  # Returns score averages, percentiles, etc.
```

### Streaming Progress
```ruby
Eval.run(..., progress: true)  # Shows progress bar
```

---

## References

- Go SDK: `braintrust-x-go/braintrust/eval/eval.go`
- Go Example: `braintrust-x-go/examples/evals/evals.go`
- Ruby Test Frameworks: RSpec (blocks) vs Minitest (classes)
